{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('linear_regression_3.csv')\n",
    "X = df.drop(columns=['y'])\n",
    "Y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalize with index preservation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Y is properly indexed\n",
    "Y_train = pd.Series(Y_train.values, index=X_train.index)\n",
    "Y_test = pd.Series(Y_test.values, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit initial model with aligned indices\n",
    "X_train_sm = sm.add_constant(X_train_scaled)\n",
    "model = sm.OLS(Y_train, X_train_sm).fit()  # Now indices match\n",
    "print(\"Initial Model Summary:\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Check VIF and remove high multi collinearity\n",
    "def calculate_vif(data):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i)\n",
    "                       for i in range(data.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "\n",
    "def remove_high_vif_iteratively(data, threshold=10, max_iter=20):\n",
    "    \"\"\"\n",
    "    Removes high-VIF features one at a time, recomputing VIF after each removal.\n",
    "    Preserves features that are statistically significant despite high VIF.\n",
    "    \"\"\"\n",
    "    for _ in range(max_iter):\n",
    "        vif = calculate_vif(data)\n",
    "        max_vif = vif['VIF'].max()\n",
    "\n",
    "        if max_vif <= threshold:\n",
    "            break\n",
    "\n",
    "        # Get the feature with highest VIF\n",
    "        worst_feature = vif.loc[vif['VIF'].idxmax(), 'feature']\n",
    "\n",
    "        # Check if the feature is statistically significant\n",
    "        temp_model = sm.OLS(Y_train, sm.add_constant(data)).fit()\n",
    "        p_value = temp_model.pvalues[worst_feature]\n",
    "\n",
    "        # Only remove if insignificant (p > 0.05)\n",
    "        if p_value > 0.05:\n",
    "            print(f\"Removing {worst_feature} (VIF={max_vif:.1f}, p={p_value:.3f})\")\n",
    "            data = data.drop(columns=[worst_feature])\n",
    "        else:\n",
    "            print(f\"Keeping {worst_feature} despite high VIF ({max_vif:.1f}) because it's significant (p={p_value:.3f})\")\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply iterative removal\n",
    "print(\"\\nStarting VIF reduction process...\")\n",
    "X_train_reduced = remove_high_vif_iteratively(X_train_scaled.copy())\n",
    "X_test_reduced = X_test_scaled[X_train_reduced.columns]\n",
    "\n",
    "# Verify final VIFs\n",
    "final_vif = calculate_vif(X_train_reduced)\n",
    "print(\"\\nFinal VIF Scores:\")\n",
    "print(final_vif)\n",
    "\n",
    "# Check model performance\n",
    "if not X_train_reduced.empty:\n",
    "    X_train_sm = sm.add_constant(X_train_reduced)\n",
    "    model_reduced = sm.OLS(Y_train, X_train_sm).fit()\n",
    "    print(f\"\\nModel R² after VIF reduction: {model_reduced.rsquared:.3f} \")\n",
    "else:\n",
    "    print(\"Warning: All features removed by VIF threshold!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Re-fit model after VIF reduction\n",
    "if not X_train_reduced.empty:\n",
    "    X_train_sm = sm.add_constant(X_train_reduced)\n",
    "    model_reduced = sm.OLS(Y_train, X_train_sm).fit()\n",
    "    print(\"\\nReduced Model Summary (after VIF):\")\n",
    "    print(model_reduced.summary())\n",
    "\n",
    "    # Check for features with borderline VIF (4-10) and high p-values\n",
    "    vif_reduced = calculate_vif(X_train_reduced)\n",
    "    p_values = model_reduced.pvalues[1:]  # Exclude intercept\n",
    "\n",
    "    # Identify candidate features for removal\n",
    "    candidate_features = []\n",
    "    for feature in X_train_reduced.columns:\n",
    "        vif_val = vif_reduced[vif_reduced['feature'] == feature]['VIF'].values[0]\n",
    "        p_val = p_values[feature]\n",
    "        if (4 < vif_val <= 10) and (p_val > 0.05):\n",
    "            candidate_features.append(feature)\n",
    "\n",
    "    # Cross-validation to decide on removing borderline features\n",
    "    current_features = X_train_reduced.columns.tolist()\n",
    "    for feature in candidate_features:\n",
    "        # Check if feature is still present\n",
    "        if feature not in current_features:\n",
    "            continue\n",
    "\n",
    "        # Cross-validation comparison\n",
    "        scores_with = cross_val_score(LinearRegression(),\n",
    "                                      X_train_reduced[current_features], Y_train,\n",
    "                                      cv=5, scoring='r2').mean()\n",
    "        scores_without = cross_val_score(LinearRegression(),\n",
    "                                         X_train_reduced[current_features].drop(columns=feature), Y_train,\n",
    "                                         cv=5, scoring='r2').mean()\n",
    "\n",
    "        if scores_without > scores_with:\n",
    "            current_features.remove(feature)\n",
    "\n",
    "    X_train_reduced = X_train_reduced[current_features]\n",
    "    X_test_reduced = X_test_reduced[current_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(data):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i)\n",
    "                       for i in range(data.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "def remove_high_vif_iteratively(data, y, threshold=10, max_iter=20):\n",
    "    \"\"\"\n",
    "    Removes high-VIF features one at a time, recomputing VIF after each removal.\n",
    "    Preserves features that are statistically significant despite high VIF.\n",
    "    \"\"\"\n",
    "    for _ in range(max_iter):\n",
    "        vif = calculate_vif(data)\n",
    "        max_vif = vif['VIF'].max()\n",
    "\n",
    "        if max_vif <= threshold:\n",
    "            break\n",
    "\n",
    "        worst_feature = vif.loc[vif['VIF'].idxmax(), 'feature']\n",
    "        temp_model = sm.OLS(y, sm.add_constant(data)).fit()\n",
    "        p_value = temp_model.pvalues[worst_feature]\n",
    "\n",
    "        if p_value > 0.05:\n",
    "            print(f\"Removing {worst_feature} (VIF={max_vif:.1f}, p={p_value:.3f})\")\n",
    "            data = data.drop(columns=[worst_feature])\n",
    "        else:\n",
    "            print(f\"Keeping {worst_feature} despite high VIF ({max_vif:.1f}) because significant (p={p_value:.3f})\")\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n",
    "def handle_borderline_vif(X, y, vif_lower=4, vif_upper=10, p_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Handles features with borderline VIF using cross-validation.\n",
    "    Only removes features if they have high p-value AND removal improves CV R².\n",
    "    \"\"\"\n",
    "    vif = calculate_vif(X)\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    p_values = model.pvalues[1:]  # Exclude intercept\n",
    "\n",
    "    candidates = []\n",
    "    for feature in X.columns:\n",
    "        vif_val = vif[vif['feature'] == feature]['VIF'].values[0]\n",
    "        p_val = p_values[feature]\n",
    "        if (vif_lower < vif_val <= vif_upper) and (p_val > p_threshold):\n",
    "            candidates.append(feature)\n",
    "\n",
    "    current_features = X.columns.tolist()\n",
    "    for feature in candidates:\n",
    "        if feature not in current_features:\n",
    "            continue\n",
    "\n",
    "        scores_with = cross_val_score(LinearRegression(),\n",
    "                                      X[current_features], y,\n",
    "                                      cv=5, scoring='r2').mean()\n",
    "        scores_without = cross_val_score(LinearRegression(),\n",
    "                                         X[current_features].drop(columns=feature), y,\n",
    "                                         cv=5, scoring='r2').mean()\n",
    "\n",
    "        if scores_without > scores_with:\n",
    "            current_features.remove(feature)\n",
    "            print(f\"Removing {feature} (VIF={vif_val:.1f}, p={p_val:.3f}) improved CV R²\")\n",
    "\n",
    "    return X[current_features]\n",
    "\n",
    "# Main VIF handling pipeline\n",
    "print(\"\\n=== Starting VIF Reduction Process ===\")\n",
    "\n",
    "# 1. First remove high VIF features iteratively\n",
    "print(\"\\nStep 1: Removing high-VIF features (VIF > 10)...\")\n",
    "X_train_reduced = remove_high_vif_iteratively(X_train_scaled.copy(), Y_train)\n",
    "X_test_reduced = X_test_scaled[X_train_reduced.columns]\n",
    "\n",
    "# 2. Handle borderline VIF features with cross-validation\n",
    "if not X_train_reduced.empty:\n",
    "    print(\"\\nStep 2: Checking borderline VIF features (4 < VIF ≤ 10)...\")\n",
    "    X_train_reduced = handle_borderline_vif(X_train_reduced, Y_train)\n",
    "    X_test_reduced = X_test_reduced[X_train_reduced.columns]\n",
    "\n",
    "# 3. Final verification\n",
    "if not X_train_reduced.empty:\n",
    "    final_vif = calculate_vif(X_train_reduced)\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(\"Remaining Features:\", X_train_reduced.columns.tolist())\n",
    "    print(\"\\nFinal VIF Scores:\")\n",
    "    print(final_vif)\n",
    "\n",
    "    final_model = sm.OLS(Y_train, sm.add_constant(X_train_reduced)).fit()\n",
    "    print(f\"\\nFinal Model R²: {final_model.rsquared:.3f}\")\n",
    "    print(\"\\nModel Summary:\")\n",
    "    print(final_model.summary())\n",
    "else:\n",
    "    print(\"Warning: All features removed during VIF reduction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not X_train_reduced.empty:\n",
    "    X_train_sm = sm.add_constant(X_train_reduced)\n",
    "    model_reduced = sm.OLS(Y_train, X_train_sm).fit()\n",
    "    influence = OLSInfluence(model_reduced)\n",
    "\n",
    "    # Get Cook's distance (returns array of distances and scalar threshold)\n",
    "    cook_d = influence.cooks_distance[0]  # Array of distances for each observation\n",
    "    cook_threshold = 4 / (len(X_train_reduced) - X_train_reduced.shape[1] - 1)  # Manual threshold calc\n",
    "\n",
    "    # Get DFFITS\n",
    "    dffits = influence.dffits[0]\n",
    "    dffits_threshold = 2 * np.sqrt((X_train_reduced.shape[1] + 1)/len(X_train_reduced))\n",
    "\n",
    "    # Identify outliers\n",
    "    outlier_mask = (cook_d > cook_threshold) | (np.abs(dffits) > dffits_threshold)\n",
    "\n",
    "    print(f\"\\nOutlier Statistics:\")\n",
    "    print(f\"• Max Cook's D: {np.max(cook_d):.4f} (Threshold: {cook_threshold:.4f})\")\n",
    "    print(f\"• Max |DFFITS|: {np.max(np.abs(dffits)):.4f} (Threshold: {dffits_threshold:.4f})\")\n",
    "    print(f\"Found {np.sum(outlier_mask)} influential outliers ({np.mean(outlier_mask):.1%} of data)\")\n",
    "\n",
    "    # Remove outliers\n",
    "    X_train_clean = X_train_reduced.loc[~outlier_mask]\n",
    "    Y_train_clean = Y_train.loc[~outlier_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Final model fitting\n",
    "if not X_train_clean.empty:\n",
    "    X_train_final = sm.add_constant(X_train_clean)\n",
    "    final_model = sm.OLS(Y_train_clean, X_train_final).fit()\n",
    "    print(\"\\nFinal Model Summary:\")\n",
    "    print(final_model.summary())\n",
    "\n",
    "    # 9. Residual analysis\n",
    "    residuals = final_model.resid\n",
    "    fitted = final_model.fittedvalues\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Residuals vs Fitted\n",
    "    plt.subplot(221)\n",
    "    plt.scatter(fitted, residuals, alpha=0.6)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Fitted')\n",
    "\n",
    "    # Q-Q Plot\n",
    "    plt.subplot(222)\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title('Normal Q-Q Plot')\n",
    "\n",
    "    # Residual Histogram\n",
    "    plt.subplot(223)\n",
    "    plt.hist(residuals, bins=30, edgecolor='k')\n",
    "    plt.title('Residual Distribution')\n",
    "\n",
    "    # Scale-Location Plot\n",
    "    plt.subplot(224)\n",
    "    plt.scatter(fitted, np.sqrt(np.abs(residuals)), alpha=0.6)\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('√|Standardized Residuals|')\n",
    "    plt.title('Scale-Location Plot')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 10. Test set evaluation\n",
    "    X_test_final = sm.add_constant(X_test_reduced[X_train_clean.columns], has_constant='add')\n",
    "    test_predictions = final_model.predict(X_test_final)\n",
    "    test_residuals = Y_test - test_predictions\n",
    "\n",
    "    # Plot test residual distribution with IQR bounds\n",
    "    Q1 = np.percentile(test_residuals, 25)\n",
    "    Q3 = np.percentile(test_residuals, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.hist(test_residuals, bins=30, edgecolor='k', alpha=0.7)\n",
    "    plt.axvline(lower_bound, color='r', linestyle='--', label='Lower Bound')\n",
    "    plt.axvline(upper_bound, color='r', linestyle='--', label='Upper Bound')\n",
    "    plt.title('Test Set Residual Distribution with IQR Bounds')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    test_mask = (test_residuals >= lower_bound) & (test_residuals <= upper_bound)\n",
    "\n",
    "    # Full test set metrics\n",
    "    test_r2 = r2_score(Y_test, test_predictions)\n",
    "    test_rmse = np.sqrt(mean_squared_error(Y_test, test_predictions))\n",
    "\n",
    "    # Clean test set metrics (non-outliers only)\n",
    "    test_r2_clean = r2_score(Y_test[test_mask], test_predictions[test_mask])\n",
    "    test_rmse_clean = np.sqrt(mean_squared_error(Y_test[test_mask], test_predictions[test_mask]))\n",
    "    outlier_percentage = (1 - test_mask.mean()) * 100\n",
    "\n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Train Adjusted R²: {final_model.rsquared_adj:.3f}\")\n",
    "    print(f\"\\nTest Set Performance (Full):\")\n",
    "    print(f\"R²: {test_r2:.3f} | RMSE: {test_rmse:.3f} | n={len(Y_test)}\")\n",
    "    print(f\"\\nTest Set Performance (Non-outliers only):\")\n",
    "    print(f\"R²: {test_r2_clean:.3f} | RMSE: {test_rmse_clean:.3f} | n={test_mask.sum()} ({100-outlier_percentage:.1f}% retained)\")\n",
    "    print(f\"\\nOutlier Statistics:\")\n",
    "    print(f\"• Residual IQR: [{Q1:.3f}, {Q3:.3f}]\")\n",
    "    print(f\"• Outlier bounds: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n",
    "    print(f\"• Outlier percentage: {outlier_percentage:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"No features remaining after feature selection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
